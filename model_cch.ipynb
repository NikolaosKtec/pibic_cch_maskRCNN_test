{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import os\n",
    "from os import listdir\n",
    "from random import shuffle\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import advance libraries\n",
    "from xml.etree import ElementTree\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT\n",
    "ROOT = os.getcwd()\n",
    "MODEL = os.path.join(ROOT,'model')\n",
    "DATASET = os.path.join(ROOT,'dataset')\n",
    "# TRAIN = os.path.join(DATASET,'train')\n",
    "# VALIDATION = os.path.join(DATASET,'validation')\n",
    "# anotation and image source\n",
    "IMAGE_DIR =  os.path.join(DATASET,'images')\n",
    "ANNOTATIONS_DIR = os.path.join(DATASET,'annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !alias pip=/usr/bin/pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.17\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import mask rcnn libraries\n",
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.utils import extract_bboxes\n",
    "from mrcnn.utils import compute_ap\n",
    "from mrcnn.model import load_image_gt\n",
    "from mrcnn.model import mold_image\n",
    "from mrcnn import visualize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>change dir and download model</h1>\n",
    "<p>use:</p>\n",
    "<span>!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy libraries\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras libraries\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# inline matplotlib\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CchDataset(Dataset):\n",
    "    #  prepare dataset and ignore images without annotations\n",
    "    def prepare_dataset(self,IMAGE_DIR, ANNOTATIONS_DIR):\n",
    "\n",
    "        images = []\n",
    "        annotations = []\n",
    "        image_id_list = []\n",
    "        images_dir_list = listdir(IMAGE_DIR)\n",
    "        shuffle(images_dir_list)\n",
    "        \n",
    "        for filename in images_dir_list:\n",
    "            \n",
    "            # extract image id\n",
    "            image_id = filename[:-4] # used to skip last 4 chars which is '.jpg' (class_id.jpg)\n",
    "            \n",
    "            \n",
    "            img_path = IMAGE_DIR +'/'+filename\n",
    "            ann_path = ANNOTATIONS_DIR+'/'+ image_id + '.xml'\n",
    "            \n",
    "            if os.path.exists(img_path) and os.path.exists(ann_path):\n",
    "\n",
    "                images.append(img_path)\n",
    "                annotations.append(ann_path)\n",
    "                image_id_list.append(image_id)\n",
    "\n",
    "            # print(f'exist? >> {os.path.exists(img_path)} >> path_ing: {img_path}')\n",
    "            # print(f'exist? >> {os.path.exists(ann_path)} >> path_ann: {ann_path}')\n",
    "            \n",
    "        return (images, annotations,image_id_list)\n",
    "\n",
    "            \n",
    "    \n",
    "    # load_dataset function is used to load the train and test dataset\n",
    "    def load_dataset(self,IMAGES_LIST, ANNOTATIONS_LIST, IMAGE_ID_LIST, is_train=True):\n",
    "        print(f'train? {is_train}')\n",
    "        # we add a class that we need to classify in our case it is Damage\n",
    "        # self.add_class(\"dataset\", 1, \"Damage\")\n",
    "        self.add_class(DATASET, 1, \"Femea\") #2 e 3 instares\n",
    "        self.add_class(DATASET, 2, \"Macho\")\n",
    "        self.add_class(DATASET, 3, \"Linfa\") #1 instar femea\n",
    "\n",
    "        LENGHT = len(IMAGES_LIST)\n",
    "        print(f'len? {LENGHT}')\n",
    "        OFFSET = math.floor(LENGHT * 0.8)\n",
    "        # print(OFFSET)\n",
    "        for i in range(LENGHT):\n",
    "            \n",
    "            \n",
    "            # regra dos 80/20 corrigido em 13.06.2023\n",
    "            if not is_train: #it is test (last 20%)\n",
    "                if i < OFFSET:\n",
    "                    # print(f'passs>> {i}')\n",
    "                    continue\n",
    "            else: # it is training\n",
    "                if i > OFFSET:\n",
    "                    # print(f'passs>> {i}')\n",
    "                    continue\n",
    "\n",
    "            # using add_image function we pass image_id, image_path and ann_path so that the current\n",
    "            # image is added to the dataset for training or testing\n",
    "            \n",
    "            self.add_image(source=DATASET,image_id=IMAGE_ID_LIST[i], path=IMAGES_LIST[i], annotation=ANNOTATIONS_LIST[i])\n",
    "            \n",
    "\n",
    "    # function used to extract bouding boxes from annotated files\n",
    "    def extract_boxes(self, filename):\n",
    "\n",
    "        # you can see how the images are annotated we extracrt the width, height and bndbox values\n",
    "        # <annotation>\n",
    "        # <size>\n",
    "        #       <width>640</width>\n",
    "        #       <height>360</height>\n",
    "        #       <depth>3</depth>\n",
    "        # </size>\n",
    "        # <object>\n",
    "        #          <name>damage</name>\n",
    "        #          <pose>Unspecified</pose>\n",
    "        #          <truncated>0</truncated>\n",
    "        #          <difficult>0</difficult>\n",
    "        #          <bndbox>\n",
    "        #                 <xmin>315</xmin>\n",
    "        #                 <ymin>160</ymin>\n",
    "        #                 <xmax>381</xmax>\n",
    "        #                 <ymax>199</ymax>\n",
    "        #          </bndbox>\n",
    "        # </object>\n",
    "        # </annotation>\n",
    "        \n",
    "        # used to parse the .xml files\n",
    "        tree = ElementTree.parse(filename)\n",
    "        \n",
    "        # to get the root of the xml file\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # we will append all x, y coordinated in boxes\n",
    "        # for all instances of an onject\n",
    "        boxes = list()\n",
    "        \n",
    "        # we find all attributes with name bndbox\n",
    "        # bndbox will exist for each ground truth in image\n",
    "        for box in root.findall('.//object'):\n",
    "            name = box.find('name').text\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax, name]\n",
    "            boxes.append(coors)\n",
    "        \n",
    "            # I have included this line to skip any un-annotated images\n",
    "            if name=='Femea' or name=='Macho' or name=='Linfa':\n",
    "                boxes.append(coors)\n",
    "\n",
    "        # extract width and height of the image\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        \n",
    "        # return boxes-> list, width-> int and height-> int \n",
    "        return boxes, width, height\n",
    "    \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # we get the annotation path of image_id which is dataset_dir/annots/image_id.xml\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # we call the extract_boxes method(above) to get bndbox from .xml file\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # we create len(boxes) number of masks of height 'h' and width 'w'\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "        \n",
    "        \n",
    "        class_ids = list()\n",
    "        \n",
    "        # we loop over all boxes and generate masks (bndbox mask) and class id for each instance\n",
    "        # masks will have rectange shape as we have used bndboxes for annotations\n",
    "        # for example:  if 2.jpg have three objects we will have following masks and class_ids\n",
    "        # 000000000 000000000 000001110 \n",
    "        # 000011100 011100000 000001110\n",
    "        # 000011100 011100000 000001110\n",
    "        # 000000000 011100000 000000000\n",
    "        #    1         1          1    <- class_ids\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            \n",
    "            \n",
    "            if (box[4] == 'Level-1'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "                class_ids.append(self.class_names.index('Femea'))\n",
    "            elif(box[4] == 'Level-2'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 2\n",
    "                class_ids.append(self.class_names.index('Macho')) \n",
    "            elif(box[4] == 'Level-3'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 3\n",
    "                class_ids.append(self.class_names.index('Linfa'))\n",
    "           \n",
    "        \n",
    "        # return masks and class_ids as array\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# damage configuration class, you can change values of hyper parameters here\n",
    "class CchConfig(Config):\n",
    "    # name of the configuration\n",
    "    NAME = \"cochonilha\"\n",
    "    \n",
    "    # damage class + background class\n",
    "    NUM_CLASSES = 3 + 1\n",
    "    \n",
    "    # steps per epoch and minimum confidence\n",
    "    STEPS_PER_EPOCH = 80\n",
    "    \n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 300\n",
    "    \n",
    "    # validation steps\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train? True\n",
      "len? 120\n",
      "train? False\n",
      "len? 120\n"
     ]
    }
   ],
   "source": [
    "# prepare train set\n",
    "train_set = CchDataset()\n",
    "#  organize data\n",
    "(images, annotations, path_ids) = train_set.prepare_dataset(IMAGE_DIR,ANNOTATIONS_DIR)\n",
    "\n",
    "train_set.load_dataset(images,annotations,path_ids,True)\n",
    "train_set.prepare()\n",
    "\n",
    "# validation/test\n",
    "test_set = CchDataset()\n",
    "test_set.load_dataset(images,annotations,path_ids,False)\n",
    "test_set.prepare()\n",
    "\n",
    "# load damage config\n",
    "config = CchConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir=MODEL, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght>> 120 >> 120 >> 120\n"
     ]
    }
   ],
   "source": [
    "print(f'lenght>> {len(images)} >> {len(annotations)} >> {len(path_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'52f3eaae-editada_flipped_B20.jpeg_quadrante_4_3'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_ids[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/c50e2e4c-editada_flipped_B20.jpeg_quadrante_3_1.jpg\n",
      "/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/52f3eaae-editada_flipped_B20.jpeg_quadrante_4_3.jpg\n"
     ]
    }
   ],
   "source": [
    "# last\n",
    "print(train_set.image_reference(96))\n",
    "print(test_set.image_reference(23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = os.path.join(ROOT,'mask_rcnn_coco.h5')\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Treine o modelo</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.002\n",
      "\n",
      "Checkpoint Path: /home/projeto_chn_/pibic_cch_maskRCNN_test/model/cochonilha20230615T1405/mask_rcnn_cochonilha_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'id': '6753a74c-editada_flipped_B20.jpeg_quadrante_2_3', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6753a74c-editada_flipped_B20.jpeg_quadrante_2_3.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/6753a74c-editada_flipped_B20.jpeg_quadrante_2_3.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "ERROR:root:Error processing image {'id': '76ee1756-editada_flipped_B20.jpeg_quadrante_1_2', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/76ee1756-editada_flipped_B20.jpeg_quadrante_1_2.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/76ee1756-editada_flipped_B20.jpeg_quadrante_1_2.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "ERROR:root:Error processing image {'id': '0f1660e2-editada_flipped_B19.jpeg_quadrante_3_1', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0f1660e2-editada_flipped_B19.jpeg_quadrante_3_1.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/0f1660e2-editada_flipped_B19.jpeg_quadrante_3_1.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "ERROR:root:Error processing image {'id': 'c50e2e4c-editada_flipped_B20.jpeg_quadrante_3_1', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/c50e2e4c-editada_flipped_B20.jpeg_quadrante_3_1.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/c50e2e4c-editada_flipped_B20.jpeg_quadrante_3_1.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "ERROR:root:Error processing image {'id': '6897bab1-editada_90_B2.jpeg_quadrante_2_3', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6897bab1-editada_90_B2.jpeg_quadrante_2_3.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/6897bab1-editada_90_B2.jpeg_quadrante_2_3.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "ERROR:root:Error processing image {'id': '50dcf8f1-editada_flipped_B19.jpeg_quadrante_4_1', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/50dcf8f1-editada_flipped_B19.jpeg_quadrante_4_1.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/50dcf8f1-editada_flipped_B19.jpeg_quadrante_4_1.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "ERROR:root:Error processing image {'id': '791018d7-editada_90_B6.jpeg_quadrante_3_2', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/791018d7-editada_90_B6.jpeg_quadrante_3_2.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/791018d7-editada_90_B6.jpeg_quadrante_3_2.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "ERROR:root:Error processing image {'id': '7b3cb247-editada_90_B3.jpeg_quadrante_1_4', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/7b3cb247-editada_90_B3.jpeg_quadrante_1_4.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/7b3cb247-editada_90_B3.jpeg_quadrante_1_4.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "ERROR:root:Error processing image {'id': 'e120b615-editada_flipped_B20.jpeg_quadrante_4_1', 'source': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset', 'path': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/e120b615-editada_flipped_B20.jpeg_quadrante_4_1.jpg', 'annotation': '/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/annotations/e120b615-editada_flipped_B20.jpeg_quadrante_4_1.xml'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1709, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\", line 1212, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 132, in load_mask\n",
      "    boxes, w, h = self.extract_boxes(path)\n",
      "  File \"/tmp/ipykernel_71423/1844594177.py\", line 103, in extract_boxes\n",
      "    xmin = int(box.find('xmin').text)\n",
      "AttributeError: 'NoneType' object has no attribute 'text'\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71423/885292799.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             layers='heads')\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2372\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m         )\n\u001b[1;32m   2376\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2009\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO problema com anotações vazias\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, \n",
    "            test_set, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=5, \n",
    "            layers='heads')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

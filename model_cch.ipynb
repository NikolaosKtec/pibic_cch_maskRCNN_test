{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import advance libraries\n",
    "from xml.etree import ElementTree\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT\n",
    "ROOT = os.getcwd()\n",
    "DATASET = os.path.join(ROOT,'dataset')\n",
    "TRAIN = os.path.join(DATASET,'train')\n",
    "VALIDATION = os.path.join(DATASET,'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/projeto_chn_/pibic_cch_maskRCNN_test/dataset'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !alias pip=/usr/bin/pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.17\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import mask rcnn libraries\n",
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.utils import extract_bboxes\n",
    "from mrcnn.utils import compute_ap\n",
    "from mrcnn.model import load_image_gt\n",
    "from mrcnn.model import mold_image\n",
    "from mrcnn import visualize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>change dir and download model</h1>\n",
    "<p>use:</p>\n",
    "<span>!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy libraries\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras libraries\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# inline matplotlib\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class CchDataset(Dataset):\n",
    "    \n",
    "    # load_dataset function is used to load the train and test dataset\n",
    "    def load_dataset(self, DATASET, is_train=True):\n",
    "        \n",
    "        # we add a class that we need to classify in our case it is Damage\n",
    "        # self.add_class(\"dataset\", 1, \"Damage\")\n",
    "        self.add_class(\"dataset\", 1, \"Femea\")\n",
    "        self.add_class(\"dataset\", 2, \"Macho\")\n",
    "        self.add_class(\"dataset\", 3, \"Linfa\")\n",
    "\n",
    "        # anotation and image source\n",
    "        IMAGE_DIR =  os.path.join(DATASET,'images')\n",
    "        ANNOTATIONS_DIR = os.path.join(DATASET,'annotations')\n",
    "        LENGHT = listdir(IMAGE_DIR).__len__()\n",
    "        OFFSET =  math.floor(LENGHT*0.8)\n",
    "\n",
    "        # print(f'offset: {offset}')\n",
    "        # is_train will be true if we our training our model and false when we are testing the model\n",
    "        for index,filename in enumerate(listdir(IMAGE_DIR)):\n",
    "            \n",
    "            # extract image id\n",
    "            image_id = filename[:-4] # used to skip last 4 chars which is '.jpg' (class_id.jpg)\n",
    "            \n",
    "            \n",
    "            img_path = IMAGE_DIR +'/'+filename\n",
    "            ann_path = ANNOTATIONS_DIR+'/'+ image_id + '.xml'\n",
    "\n",
    "            # print(f'exist? >> {os.path.exists(img_path)} >> path_ing: {img_path}')\n",
    "            # print(f'exist? >> {os.path.exists(ann_path)} >> path_ann: {ann_path}')\n",
    "\n",
    "            # regra dos 80/20\n",
    "            if not is_train:\n",
    "                if index > OFFSET:\n",
    "                    # print(f'passs>> {index}')\n",
    "                    continue\n",
    "            else:\n",
    "                if index < OFFSET:\n",
    "                    # print(f'passs>> {index}')\n",
    "                    continue\n",
    "\n",
    "            # using add_image function we pass image_id, image_path and ann_path so that the current\n",
    "            # image is added to the dataset for training or testing\n",
    "\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    # function used to extract bouding boxes from annotated files\n",
    "    def extract_boxes(self, filename):\n",
    "\n",
    "        # you can see how the images are annotated we extracrt the width, height and bndbox values\n",
    "        # <annotation>\n",
    "        # <size>\n",
    "        #       <width>640</width>\n",
    "        #       <height>360</height>\n",
    "        #       <depth>3</depth>\n",
    "        # </size>\n",
    "        # <object>\n",
    "        #          <name>damage</name>\n",
    "        #          <pose>Unspecified</pose>\n",
    "        #          <truncated>0</truncated>\n",
    "        #          <difficult>0</difficult>\n",
    "        #          <bndbox>\n",
    "        #                 <xmin>315</xmin>\n",
    "        #                 <ymin>160</ymin>\n",
    "        #                 <xmax>381</xmax>\n",
    "        #                 <ymax>199</ymax>\n",
    "        #          </bndbox>\n",
    "        # </object>\n",
    "        # </annotation>\n",
    "        \n",
    "        # used to parse the .xml files\n",
    "        tree = ElementTree.parse(filename)\n",
    "        \n",
    "        # to get the root of the xml file\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # we will append all x, y coordinated in boxes\n",
    "        # for all instances of an onject\n",
    "        boxes = list()\n",
    "        \n",
    "        # we find all attributes with name bndbox\n",
    "        # bndbox will exist for each ground truth in image\n",
    "        for box in root.findall('.//object'):\n",
    "            name = box.find('name').text\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax, name]\n",
    "            boxes.append(coors)\n",
    "        \n",
    "            # I have included this line to skip any un-annotated images\n",
    "            if name=='Femea' or name=='Macho' or name=='Linfa':\n",
    "                boxes.append(coors)\n",
    "\n",
    "        # extract width and height of the image\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        \n",
    "        # return boxes-> list, width-> int and height-> int \n",
    "        return boxes, width, height\n",
    "    \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # we get the annotation path of image_id which is dataset_dir/annots/image_id.xml\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # we call the extract_boxes method(above) to get bndbox from .xml file\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # we create len(boxes) number of masks of height 'h' and width 'w'\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "        \n",
    "        \n",
    "        class_ids = list()\n",
    "        \n",
    "        # we loop over all boxes and generate masks (bndbox mask) and class id for each instance\n",
    "        # masks will have rectange shape as we have used bndboxes for annotations\n",
    "        # for example:  if 2.jpg have three objects we will have following masks and class_ids\n",
    "        # 000000000 000000000 000001110 \n",
    "        # 000011100 011100000 000001110\n",
    "        # 000011100 011100000 000001110\n",
    "        # 000000000 011100000 000000000\n",
    "        #    1         1          1    <- class_ids\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            \n",
    "            \n",
    "            if (box[4] == 'Level-1'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "                class_ids.append(self.class_names.index('Femea'))\n",
    "            elif(box[4] == 'Level-2'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 2\n",
    "                class_ids.append(self.class_names.index('Macho')) \n",
    "            elif(box[4] == 'Level-3'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 3\n",
    "                class_ids.append(self.class_names.index('Linfa'))\n",
    "           \n",
    "        \n",
    "        # return masks and class_ids as array\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>teste o codigo acima</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/666392d9-editada_90_B3.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/9f254806-editada_90_B7.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/660692fd-editada_90_B2.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/f4cf9830-editada_flipped_B21.jpeg_quadrante_4_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/a965b8aa-editada_180_B3.jpeg_quadrante_3_4.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/9ce02bec-editada_flipped_B22.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/215e4820-editada_flipped_B21.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/a992098e-editada_90_B1.jpeg_quadrante_4_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/99a060bd-editada_90_B6.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/60fdd446-editada_flipped_B21.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/a1fd2853-editada_90_B2.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/a4d36101-editada_flipped_B20.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/f7a55254-editada_flipped_B22.jpeg_quadrante_4_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/b183f89a-editada_90_B7.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/65796b57-editada_flipped_B22.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/da3e58a2-editada_90_B4.jpg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/31567983-editada_90_B8.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0bef67a9-editada_flipped_B21.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/07b7ee8e-editada_flipped_B22.jpeg_quadrante_4_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/93793ce9-editada_90_B6.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/5f4b595f-editada_90_B3.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/5b0d1b3d-editada_flipped_B20.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/c1b7cc84-editada_90_B1.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/ea558a62-editada_90_B6.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/16ad38e9-editada_90_B3.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0a5654a5-editada_90_B2.jpeg_quadrante_4_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/62c07f6a-editada_flipped_B21.jpeg_quadrante_4_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/f5d7b4ef-editada_flipped_B22.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/cd075b95-editada_180_B6.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/cddd6fb5-editada_flipped_B19.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6cd5c833-editada_90_B3.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/81cda7f4-editada_90_B6.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/22c30912-editada_flipped_B21.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/4bd7b504-editada_90_B2.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0dee4495-editada_90_B1_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/754a055f-editada_flipped_B22.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/10e1f346-editada_90_B3.jpeg_quadrante_3_5.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0298d47b-editada_90_B7.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/7a64c3bc-editada_90_B3.jpeg_quadrante_3_4.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/2d89555d-editada_180_B3.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/ecbb2ea1-editada_90_B6.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/309c1e87-editada_90_B3.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/302e32ce-editada_flipped_B22.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/7f6416ce-editada_180_B3.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/ae019f71-editada_90_B1.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/66fc3304-editada_90_B6.jpeg_quadrante_4_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/9dc800f8-editada_flipped_B21.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/35aa4c13-editada_180_B6.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/8ce43b66-editada_90_B3.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/f706b73e-editada_flipped_B21.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/de3b99fb-editada_flipped_B19.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/ec59a95c-editada_flipped_B22.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/49971ba4-editada_90_B7.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/9fb056af-editada_90_B6.jpeg_quadrante_4_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0fe7c87d-editada_flipped_B21.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/11ed5b02-editada_180_B3.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6600c6e1-editada_flipped_B19.jpeg_quadrante_4_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/aa9b7f59-editada_90_B3.jpeg_quadrante_1_5.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/01fc0bb3-editada_90_B7.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/52f3eaae-editada_flipped_B20.jpeg_quadrante_4_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/fc49505a-editada_90_B1.jpeg_quadrante_4_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/2481f83e-editada_flipped_B22.jpeg_quadrante_4_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/b1f1f36d-editada_90_B7.jpeg_quadrante_4_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/fd148c33-editada_flipped_B19.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6ab189b2-editada_90_B1.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/774b7fcb-editada_180_B3.jpeg_quadrante_2_5.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/8b257c8a-editada_90_B6.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/58bf778d-editada_flipped_B20.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0813887f-editada_90_B2.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6897bab1-editada_90_B2.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/866526e8-editada_90_B7.jpeg_quadrante_4_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/efe27673-editada_90_B6.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0f1660e2-editada_flipped_B19.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/c50e2e4c-editada_flipped_B20.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/fda775b3-editada_90_B3.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/7779218b-editada_180_B3.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/18d88646-editada_90_B2.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6c9a6300-editada_180_B3.jpeg_quadrante_1_4.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/f87084e8-editada_90_B1.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/1524b419-editada_90_B3.jpeg_quadrante_2_5.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/8d3d5f7b-editada_flipped_B20.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/90b9407c-editada_flipped_B19.jpeg_quadrante_4_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/f33081f9-editada_flipped_B22.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/e4209064-editada_flipped_B19.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/0c9cbcb3-editada_90_B3.jpeg_quadrante_2_4.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/d79d192e-editada_90_B7.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/76ee1756-editada_flipped_B20.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/39c0741a-editada_90_B5.jpg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/ba8abb59-editada_90_B2.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/d352e737-editada_180_B3.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/4efda941-editada_90_B1.jpeg_quadrante_2_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/c4f2ed4f-editada_90_B3.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/434358ec-editada_flipped_B20.jpeg_quadrante_4_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/3f97fba9-editada_flipped_B22.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/4a6dc128-editada_flipped_B20.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/5f2b4211-editada_90_B8.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/09fdaa07-editada_flipped_B20.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/4f5f23bc-editada_180_B3.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/1c008608-editada_90_B6.jpeg_quadrante_4_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/e8dfc75c-editada_90_B7.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/50dc8e5c-editada_flipped_B19.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/b776cf70-editada_90_B2.jpeg_quadrante_3_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/50dcf8f1-editada_flipped_B19.jpeg_quadrante_4_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/e060e893-editada_flipped_B21.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/7f50ec89-editada_flipped_B22.jpeg_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/803e0915-editada_90_B2.jpeg_quadrante_4_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/791018d7-editada_90_B6.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/4d6ac6fc-editada_90_B2.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/3d87257d-editada_180_B3.jpeg_quadrante_2_4.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/7b3cb247-editada_90_B3.jpeg_quadrante_1_4.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/546c04b7-editada_90_B7.jpeg_quadrante_4_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/362ae812-editada_180_B3.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/ad4d46ea-editada_180_B3.jpeg_quadrante_3_5.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6753a74c-editada_flipped_B20.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/f60e38dd-editada_90_B7.jpeg_quadrante_1_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/a2353c80-editada_90_B1.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/18ad9874-editada_180_B3.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/f4963a63-editada_90_B7.jpeg_quadrante_2_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/42d9fbb8-editada_90_B1.jpeg_quadrante_2_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/36836ae1-editada_flipped_B21.jpeg_quadrante_3_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/ff006236-editada_90_B1_quadrante_1_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/6a5e0a45-editada_90_B1.jpeg_quadrante_4_3.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/d4529a6f-editada_90_B8.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/7f0e20a7-editada_180_B3.jpeg_quadrante_1_5.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/ccfbf871-editada_flipped_B21.jpeg_quadrante_4_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/8e944752-editada_90_B2.jpeg_quadrante_4_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/1f9521fc-editada_90_B6.jpeg_quadrante_1_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/e120b615-editada_flipped_B20.jpeg_quadrante_4_1.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/340cfcd3-editada_90_B3.jpeg_quadrante_3_2.jpg\n",
      "exist? >> True >> path_ing: /home/projeto_chn_/pibic_cch_maskRCNN_test/dataset/images/582ed313-editada_180_B3.jpeg_quadrante_3_1.jpg\n"
     ]
    }
   ],
   "source": [
    "cch_data_class = CchDataset()\n",
    "cch_data_class.load_dataset(DATASET,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# damage configuration class, you can change values of hyper parameters here\n",
    "class CchConfig(Config):\n",
    "    # name of the configuration\n",
    "    NAME = \"conchonilha\"\n",
    "    \n",
    "    # damage class + background class\n",
    "    NUM_CLASSES = 3 + 1\n",
    "    \n",
    "    # steps per epoch and minimum confidence\n",
    "    STEPS_PER_EPOCH = 80\n",
    "    \n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 300\n",
    "    \n",
    "    # validation steps\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO CONTINUE... \n",
    "# load the train dataset\n",
    "train_set = CchDataset()\n",
    "train_set.load_dataset('customImages/stage-1', is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "# load the test dataset\n",
    "test_set = CchDataset()\n",
    "test_set.load_dataset('customImages/stage-1', is_train=False)\n",
    "test_set.prepare()\n",
    "\n",
    "# prepare config by calling the user defined confifuration class\n",
    "config = CchConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = 'mask_rcnn_coco.h5'\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
